{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3\n",
    "\n",
    "## Monte Carlo Tree Search\n",
    "\n",
    "In this mini project we will implement a recently developed planning method called monte carlo tree search. This algorithm combines the strengths of a structured tree search with the exploratory behavior of random \"rollouts\" to focus on promising parts of the state space, and has been particularly successful with perfect information games using a large state space (such as go). Similarly to RL, planning has to balance exploration and exploitation and MCTS employs a number of heuristics to try to do it.\n",
    "\n",
    "\n",
    "\n",
    "As you can see from the pseudocode, the algorithm has three key components: a tree policy (for action selection within the search tree), a default policy (for rollouts outside the search tree), and a backup function for backing up accumulated rewards over the simulated episode. We will first implement an example for each of these, and then put them together into the complete algorithm.\n",
    "\n",
    "A version in the algorithm is summerized in the following pseudocode:\n",
    "\n",
    "<img src=\"mcts_pseudocode.png\",width='500'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import a generic graph structure with state nodes and action nodes, and implement a set of classes defining corresponding states and actions using the code below, aimed to integrate RL worlds from module 2 (in this case the windy cliff world). State nodes have these state objects as attributes, which in turn have position attribute returning the location in the maze. \n",
    "\n",
    "Action objects have the attribute move, which tell you the direction (0 to 3) corresponding to that action. You can change, and extend the reward function later to better solve the task. Please study the code below and graph.py for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import RL_worlds\n",
    "from RL_worlds import windy_cliff_grid_2\n",
    "from RL_worlds import windy_cliff_grid\n",
    "from graph import *\n",
    "from default_policies import RandomKStepRollOut\n",
    "import random\n",
    "import utils\n",
    "#from backups import monte_carlo\n",
    "class MazeAction(object):\n",
    "    def __init__(self, move):\n",
    "        self.move = np.asarray(move)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.move == other.move\n",
    "        \n",
    "   \n",
    "class MazeState(object):\n",
    "    def __init__(self, pos):\n",
    "        self.pos = np.asarray(pos)\n",
    "        self.actions = [MazeAction(0),\n",
    "                        MazeAction(1),\n",
    "                        MazeAction(2),\n",
    "                        MazeAction(3)]\n",
    "    \n",
    "    def perform(self, action):\n",
    "        pos,r = wd.get_outcome(self.pos,action.move)\n",
    "        \n",
    "        return MazeState(pos)\n",
    "        \n",
    "    def reward(self, parent, action):\n",
    "         if self.pos in [53,131]:\n",
    "             \n",
    "             return 10000\n",
    "         elif self.pos in [2,3,4,28,42,56,70]:\n",
    "             \n",
    "            return -10000\n",
    "         elif self.pos in rstates[len(rstates)-5:]:#== parent.pos:\n",
    "             \n",
    "             return -200\n",
    "         return -10\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "    def is_terminal(self):\n",
    "        if (self.pos in [2,3,4, 53,131,28,42,56,70]):\n",
    "           \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "             \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you created a search tree, you can navigate around it using the following syntax (which we highlight with a list of examples):\n",
    "To see the maze location corresponding to a node: node.state.pos\n",
    "To see the visitation counts: node.n\n",
    "The possible actions from a state node are the children nodes node.children.values()\n",
    "So to see how many times during the rollouts you took action 0 from the root node: root.children.values()[0].n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "\n",
    "Write a function called ucbt to execute the UCB (upper confidence bound) tree poilcy. In particular write a function that takes as input an action-node, and returns the ucb value, $Q(s,a)_{ucb}=Q(s,a)+c * \\sqrt {\\frac{\\log n(s)}{n(s,a)}}$, where n() denotes the number of visitations to the state, or action node. UCB uses optimism in the face of uncertainty, and chooses the action that could potentially be the best, given the remaining uncertainty about its value (specified in terms of the number of previous visitations). In this sense it's a bandit algorithm with Bayesian inspiration.\n",
    "\n",
    "Write a default policy function that performs a k-step random rollout (picking a random action at each step for k steps), but stops if a state is terminal. The function should return the total accumulated discounted reward.\n",
    "\n",
    "Write a backup function, that takes as input the terminal node and adjusts the value of all nodes (state and action) visited in the episode (this just means walking up the search tree). You can assume that the result of the rollout from the terminal node has been stored in node.reward. The rewards for actions taken while in the search tree can either be computed inside the backup function, or stored during the tree traversal during the episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stop_k=500\n",
    "c=5000\n",
    "\n",
    "def ucbt(action_node):\n",
    "     return (action_node.q +\n",
    "                c * np.sqrt(2 * np.log(action_node.parent.n) /\n",
    "                                 action_node.n))\n",
    " \n",
    "\n",
    "\n",
    "'''\n",
    "class RandomKStepRollOut(object):\n",
    "   \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, state_node):\n",
    "        self.current_k = 0\n",
    "\n",
    "        def stop_k_step(state):\n",
    "            self.current_k += 1\n",
    "            return self.current_k > self.k or state.is_terminal()\n",
    "\n",
    "        return _roll_out(state_node, stop_k_step)\n",
    "    \n",
    "def random_terminal_roll_out(state_node):\n",
    "    \"\"\"\n",
    "    Estimate the reward with the sum of a rollout till a terminal state.\n",
    "    Typical for terminal-only-reward situations such as games with no\n",
    "    evaluation of the board as reward.\n",
    "\n",
    "    :param state_node:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def stop_terminal(state):\n",
    "        return state.is_terminal()\n",
    "\n",
    "    return _roll_out(state_node, stop_terminal)\n",
    "\n",
    "\n",
    "def _roll_out(state_node, stopping_criterion):\n",
    "    gamma=0.99\n",
    "    state = state_node.state\n",
    "    parent = state_node.parent.parent.state\n",
    "    action = state_node.parent.action\n",
    "    reward = state.reward(parent,action)\n",
    "    #print('r',reward)\n",
    "    while not stopping_criterion(state):\n",
    "        #print('rr',reward)\n",
    "        \n",
    "\n",
    "        action = np.random.choice(state_node.state.actions)\n",
    "        parent = state\n",
    "        state = parent.perform(action)\n",
    "        reward += gamma*state.reward(parent, action)\n",
    "        gamma*=0.99\n",
    "    return reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Random_k_Rollout(state_node):\n",
    "    k=0\n",
    "    gamma=0.99\n",
    "    state = state_node.state\n",
    "    parent = state_node.parent.parent.state\n",
    "    action = state_node.parent.action\n",
    "    reward = state.reward(parent,action)\n",
    "    #print('r',reward)\n",
    "    while not (k>stop_k or state.is_terminal()):\n",
    "        k+=1\n",
    "        \n",
    "        \n",
    "\n",
    "        action = np.random.choice(state_node.state.actions)\n",
    "        parent = state\n",
    "        state = parent.perform(action)\n",
    "        #print state.pos\n",
    "        #print state.reward(parent,action)\n",
    "        reward += gamma*state.reward(parent, action)\n",
    "        \n",
    "        gamma*=0.99\n",
    "    return reward\n",
    "\n",
    "'''\n",
    "def monte_carlo(node, depth):\n",
    "    \n",
    "    r = node.reward\n",
    "    while node is not None:\n",
    "        gamma=0.99\n",
    "        disc=1\n",
    "        node.n += 1\n",
    "        node.q = ((node.n - 1)/node.n) * node.q + 1/node.n * r*disc\n",
    "        node = node.parent\n",
    "        disc*=gamma     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write the core algorithm for MCTS.\n",
    "<img src=\"basic_algorithm.png\",width='600'>\n",
    "\n",
    "We can write three helper functions:\n",
    "\n",
    "best_child should return, given a state node and the tree policy, a successor state when taking the maximum value action.\n",
    "\n",
    "expand should return ,given a state node as input, a successor state after randomly taking one of the untried actions.\n",
    "\n",
    "Finally get_next_node should take as input a state node and the tree policy, and return a successor state node using best_child while inside the search tree (no untried actions), or using expand when reaching a leaf of the search tree (The state node has untried actions.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_next_node(state_node, tree_policy):\n",
    "    #Find a node that has untried children nodes\n",
    "    while not state_node.state.is_terminal():\n",
    "        depth=0\n",
    "        if state_node.untried_actions:\n",
    "            #expand the tree \n",
    "            return expand(state_node)\n",
    "        else:\n",
    "            depth+=1\n",
    "            state_node = best_child(state_node, tree_policy)\n",
    "            \n",
    "    return state_node\n",
    "\n",
    "def best_child(state_node, tree_policy):\n",
    "    #print \"children\" ,state_node.children[0].state.pos\n",
    "    #print state_node.children.values()\n",
    "    best_action_node = utils.rand_max(state_node.children.values(),\n",
    "                                      key=tree_policy)\n",
    "    return best_action_node.sample_state()\n",
    "\n",
    "def expand(state_node):\n",
    "    action = np.random.choice(state_node.untried_actions)\n",
    "    return state_node.children[action].sample_state()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class MCTS(object):\n",
    "    \n",
    "    def __init__(self, tree_policy, default_policy, backup):\n",
    "        self.tree_policy = tree_policy\n",
    "        self.default_policy = default_policy\n",
    "        self.backup = backup\n",
    "\n",
    "    def __call__(self, root, n=500):\n",
    "       \n",
    "        depth=0\n",
    "        if root.parent is not None:\n",
    "            raise ValueError(\"Root's parent must be None.\")\n",
    "\n",
    "        for _ in range(n):\n",
    "            node = get_next_node(root, self.tree_policy)\n",
    "            node.reward = self.default_policy(node)\n",
    "            self.backup(node,depth)\n",
    "\n",
    "        return utils.rand_max(root.children.values(), key=lambda x: x.q).action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions can then be integrated into a callable Tree search object (MCTS here), where you assign you should input your own tree policy, default policy and backup functions at the time of initialization. utils.rand_max is provided to randomly choose between possible actions of equal value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCTS(object):\n",
    "    \n",
    "    def __init__(self, tree_policy, default_policy, backup):\n",
    "        self.tree_policy = tree_policy\n",
    "        self.default_policy = default_policy\n",
    "        self.backup = backup\n",
    "\n",
    "    def __call__(self, root, n=500):\n",
    "       \n",
    "        depth=0\n",
    "        if root.parent is not None:\n",
    "            raise ValueError(\"Root's parent must be None.\")\n",
    "\n",
    "        for _ in range(n):\n",
    "            node = get_next_node(root, self.tree_policy)\n",
    "            node.reward = self.default_policy(node)\n",
    "            self.backup(node,depth)\n",
    "\n",
    "        return utils.rand_max(root.children.values(), key=lambda x: x.q).action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your tree search on the windy cliff world problem. There are many moving parts you can adjust, including the reward structure (though you are not supposed to introduce new location specific rewards), any discounting you introduce, the constant for the UCB algorithm. You can also vary how the backup is performed.\n",
    "\n",
    "Try to tune your algorithm so it can reach one of the gold states from any starting state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_c=5000\n",
    "rollout_n=500\n",
    "\n",
    "mctsa=MCTS(tree_policy=ucbt, \n",
    "            default_policy=RandomKStepRollOut(rollout_n),\n",
    "            backup=monte_carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps=1\n",
    "total_steps=50\n",
    "rollout_n=500\n",
    "start_state=162\n",
    "rstates=[start_state]\n",
    "wd=windy_cliff_grid_2()\n",
    "new_state=start_state\n",
    "rstates=[start_state]\n",
    "ractions=[]\n",
    "while not MazeState(new_state).is_terminal() and nsteps<=total_steps:\n",
    "    nsteps+=1\n",
    "    root=StateNode(None,MazeState(new_state))\n",
    "    best_action = mctsa(root)\n",
    "    new_state,rreward=wd.get_outcome(root.state.pos,best_action.move)\n",
    "    rstates.append(new_state)\n",
    "    ractions.append(best_action)\n",
    "    print('S:', new_state,'A:',best_action.move, 'R:', rreward,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
