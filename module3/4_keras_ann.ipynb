{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence, Depth and High-dimensional data\n",
    "# Keras ANN notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce the MNIST dataset, and present basic methods to setup and train a *artificial neural network (ANN)* with the Keras package.\n",
    "\n",
    "The following elements will be presented:\n",
    "\n",
    "* MNIST dataset\n",
    "* data pre-processing: size reduction, scaling\n",
    "* shallow ann: setup and training \n",
    "* loss and accuracy graphs\n",
    "* optimizer options: learning rate\n",
    "* object-oriented interface\n",
    "* deep ann: setup and training\n",
    "* generating network architecture graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* THE MNIST DATABASE of handwritten digits(http://yann.lecun.com/exdb/mnist/)\n",
    "* [Are we there yet?](http://rodrigob.github.io/are_we_there_yet/build/)\n",
    "* [Keras](https://keras.io/): The Python Deep Learning library\n",
    "* Getting started with the Keras [functional API](https://keras.io/getting-started/functional-api-guide/)\n",
    "* Getting started with the Keras Sequential model ([object oriented](https://keras.io/getting-started/sequential-model-guide/))\n",
    "* Keras [model visualization](https://keras.io/visualization/)\n",
    "* An [overview](http://sebastianruder.com/optimizing-gradient-descent/) of gradient descent optimization algorithms\n",
    "* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "* colah's [blog](http://colah.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please execute the cell bellow in order to initialize the notebook environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import mod3\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (5.0, 4.0), 'lines.linewidth': 2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has the following properties:\n",
    "* fixed-size images of handwritten digits\n",
    "* images are size-normalized and centered\n",
    "* training set of 60,000 samples, test set of 10,000 samples\n",
    "* test samples are from different writters\n",
    "\n",
    "The MNIST dataset has enough complexity to apply key machine learning concepts while not being too computationaly intensive to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1**\n",
    "\n",
    "The Keras framework provides access to several popular datasets, with the module `keras.datasets`. The MNIST dataset is loaded as follows:\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "Load the MNIST dataset and review its basic properties\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* load the MNIST dataset with `mnist.load_data()`\n",
    "* print the shapes of `x_train`, `y_train`, `x_test` and `y_test`\n",
    "* plot a few random samples from the training set using `plt.imshow(img, cmap=plt.cm.gray)`\n",
    "* plot the distribution of pixel values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[Dataset properties]\n",
    "train set data shape: (60000, 28, 28)\n",
    "train set label shape: (60000,)\n",
    "test set data shape: (10000, 28, 28)\n",
    "test set label shape: (10000,)\n",
    "\n",
    "[Sample properties]\n",
    "label: 5\n",
    "shape: (28, 28)\n",
    "min: 0\n",
    "max: 255\n",
    "```\n",
    "<img src=\"fig/keras_ann_mnist.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">\n",
    "<img src=\"fig/keras_ann_mnist_hist.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2**\n",
    "\n",
    "Several pre-processing steps should take place before training the MNIST dataset.\n",
    "\n",
    "* reduce pixel count by factor of 4, in order to train the dataset on CPU\n",
    "* scale pixel intensities between 0 and 1\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* drop one in every two pixels with smart indexing kung-fu \n",
    "* scale pixel intensities between 0 and 1\n",
    "* print the shapes of `x_train`, `y_train`, `x_test` and `y_test`\n",
    "* plot a few samples\n",
    "* plot the distribution of the pixel values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "train set shape: (60000, 14, 14)\n",
    "test set shape: (60000, 14, 14)\n",
    "```\n",
    "<img src=\"fig/keras_ann_mnist_small.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">\n",
    "<img src=\"fig/keras_ann_mnist_hist_scaled.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/keras_ann_shallow_schema.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting dataset format for ANN encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph above, the inputs to the ANN are arranged as a vector of pixels, and the outputs are arranged as a vector of labels. The outputs are encoded as *1-out-of-n*, where all units are $0$ except for the unit corresponding to the class, i.e. label $2$ is encoded as $(0, 0, 1, 0, 0, 0, 0, 0, 0, 0)$.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* use function `keras.utils.to_categorical()` to transform the labels to *1-out-of-n* encoding\n",
    "* transform the samples to the required shape with smart indexing kung-fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "train set data shape: (60000, 196)\n",
    "train set label shape: (10,)\n",
    "test set data shape: (10000, 196)\n",
    "test set label shape: (10,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a shallow ANN uses the following elements:\n",
    "\n",
    "* `keras.layersInput()` sets the input\n",
    "* `keras.layersDense()` adds a fully connected layer\n",
    "* `keras.models.Model()` defines the ANN model\n",
    "* `compile()` method of `keras.models.Model` implements the ANN in Tensorflow\n",
    "* `summary()` method of `keras.models.Model` prints the ANN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc_1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           (None, 196)               0         \n",
    "_________________________________________________________________\n",
    "fc_1 (Dense)                 (None, 256)               50432     \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 10)                2570      \n",
    "=================================================================\n",
    "Total params: 53,002.0\n",
    "Trainable params: 53,002.0\n",
    "Non-trainable params: 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN is trained by calling the method `fit()` and specifying the train set. The trained model is evaluated by calling the method `evaluate()` and the specifying test set.\n",
    "\n",
    "The default named parameters of the `fit()` method are the following:\n",
    "* `batch_size=32`\n",
    "* `epochs=1`\n",
    "* `verbose=1`\n",
    "* `shuffle=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(input_train, labels_train)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Train parameters]')\n",
    "for item in history.params:\n",
    "    print(item+':', history.params[item])\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], evaluation[0])\n",
    "print('test', history.params['metrics'][1], evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/1\n",
    "60000/60000 [==============================] - 4s - loss: 0.0864 - acc: 0.3356     \n",
    "\n",
    "[Train parameters]\n",
    "metrics: ['loss', 'acc']\n",
    "samples: 60000\n",
    "batch_size: 32\n",
    "epochs: 1\n",
    "do_validation: False\n",
    "verbose: 1\n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0709026087403\n",
    "test acc 0.4896\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing default train parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the ANN under the following conditions:\n",
    "\n",
    "* 5 training epochs\n",
    "* batch size of 128\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Call the `fit()` method with relevant names parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0692 - acc: 0.5123     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0656 - acc: 0.5555     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0623 - acc: 0.5889     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0594 - acc: 0.6194     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0568 - acc: 0.6465     \n",
    "\n",
    "[Train parameters]\n",
    "metrics: ['loss', 'acc']\n",
    "samples: 60000\n",
    "batch_size: 128\n",
    "epochs: 5\n",
    "do_validation: False\n",
    "verbose: 1\n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0551402773798\n",
    "test acc 0.6666\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the ANN keeps updating its weights with each call to the `fit()` method.\n",
    "Resetting the network requires to redefine its structure and compiling.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Redefine the network, compile and train\n",
    "* compare the performance of 1 epoch vs 5 epochs training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the learning rate $eta$ requires creating an optimizer instance,\n",
    "and passing it to the relevant optimizer option:\n",
    "```\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=2)\n",
    "```\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* create an optimizer instance, set $\\eta=2$, and pass it to the `compile()` method\n",
    "* retrain network for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0311 - acc: 0.8160     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0167 - acc: 0.9106     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0142 - acc: 0.9233     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0129 - acc: 0.9309     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0122 - acc: 0.9337     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0118559889253\n",
    "test acc 0.9342\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy visualisation (Tensorboard)\n",
    "\n",
    "Open up a terminal and start a TensorBoard server that will read logs stored at `/tmp/autoencoder`.\n",
    "\n",
    "`tensorboard --logdir=/tmp/ann`\n",
    "\n",
    "This allows us to monitor training in the TensorBoard web interface at http://127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc_1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size,\n",
    "                    validation_data=(input_test, labels_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[TensorBoard(log_dir='/tmp/ann')])\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], evaluation[0])\n",
    "print('test', history.params['metrics'][1], evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/tensorboard_ann_1.png\" style=\"display:inline;margin:1px\"><img src=\"fig/tensorboard_ann_2.png\" style=\"display:inline;margin:1px\">\n",
    "<img src=\"fig/tensorboard_ann_3.png\" style=\"display:inline;margin:1px\"><img src=\"fig/tensorboard_ann_4.png\" style=\"display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy visualisation\n",
    "\n",
    "We also provide you with visualization code but first set the training loop correctly. The `fit()` function only trains over at least 1 epoch, and we need it to train over a fractions of epoch to visualise the intermediate results\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* use the code below to set the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "n_epochs = 2\n",
    "n_eval = 0.2\n",
    "n_chunk = int(n_eval*n)\n",
    "\n",
    "indexes = np.arange(n)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    #  np.random.shuffle(indexes)\n",
    "    \n",
    "    # insert looping kung-fu here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[0 1 2 3 4 5 6 7 8 9]\n",
    "[10 11 12 13 14 15 16 17 18 19]\n",
    "[20 21 22 23 24 25 26 27 28 29]\n",
    "[30 31 32 33 34 35 36 37 38 39]\n",
    "[40 41 42 43 44 45 46 47 48 49]\n",
    "[0 1 2 3 4 5 6 7 8 9]\n",
    "[10 11 12 13 14 15 16 17 18 19]\n",
    "[20 21 22 23 24 25 26 27 28 29]\n",
    "[30 31 32 33 34 35 36 37 38 39]\n",
    "[40 41 42 43 44 45 46 47 48 49]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is done, insert your loop below\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* insert your training loop below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup network\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "n = input_train.shape[0]\n",
    "n_eval = 0.2\n",
    "n_chunk = int(n_eval*n)\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc_1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "evaluate_train = model.evaluate(input_train, labels_train, verbose=0)\n",
    "evaluate_test = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "loss_train = [evaluate_train[0]]\n",
    "loss_test = [evaluate_test[0]]\n",
    "accuracy_train = [evaluate_train[1]]\n",
    "accuracy_test = [evaluate_test[1]]\n",
    "\n",
    "x_range = [0]\n",
    "\n",
    "fig=plt.figure(figsize=(9, 3))\n",
    "\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "\n",
    "fig1 = plt.subplot(gs[0])\n",
    "plt.plot(x_range, loss_train, 'C0', alpha=0.8, label='loss train')\n",
    "plt.plot(x_range, loss_test, 'C1', alpha=0.8, label='loss test')\n",
    "\n",
    "plt.ylim([0, max(loss_train[0], loss_test[0])*1.1])\n",
    "plt.xlim([0, n_epochs*1.1])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch (n)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "fig2 = plt.subplot(gs[1])\n",
    "plt.plot(x_range, loss_train, 'C0', alpha=0.8, label='loss train')\n",
    "plt.plot(x_range, loss_test, 'C1', alpha=0.8, label='loss test')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, n_epochs*1.1])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch (n)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "indexes = np.arange(n)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(indexes)\n",
    "\n",
    "    # insert looping kung-fu here\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], evaluation[0])\n",
    "print('test', history.params['metrics'][1], evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "<img src=\"fig/ann_keras_plot_custom.png\">\n",
    "\n",
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[Model evaluation]\n",
    "test loss 0.0247837326184\n",
    "test acc 0.8787\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object oriented interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an object oriented (OO) interface, as you can see in the exemple below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# setup network\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=n_fc1, input_dim=input_train_shape[0]))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(units=n_out))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/keras_ann_deep_schema.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setup and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers to the network is done by stacking layers before the output layer.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Add two additional fully connected layers to the ANN\n",
    "* set layer size to 64\n",
    "* set batch size to 32 and $\\eta$ to 3\n",
    "* retrain with  batch size of 128 and observe the difference in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT (with batch size = 32)**\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           (None, 196)               0         \n",
    "_________________________________________________________________\n",
    "fc_1 (Dense)                 (None, 64)                12608     \n",
    "_________________________________________________________________\n",
    "fc_2 (Dense)                 (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "fc_3 (Dense)                 (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 21,578.0\n",
    "Trainable params: 21,578.0\n",
    "Non-trainable params: 0.0\n",
    "_________________________________________________________________\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0901 - acc: 0.1224     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0765 - acc: 0.3522     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0396 - acc: 0.7501     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0239 - acc: 0.8580     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0187 - acc: 0.8847     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0165417414045\n",
    "test acc 0.8971\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do you MNIST me?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a competition for training MNIST. Any changes are accepted under the following \n",
    "\n",
    "**CCNSS MNIST competition manifesto **\n",
    "We abide to the the rules of training MNIST under the following conditions:\n",
    "* results are reported by averaging 5 training sessions\n",
    "* only SGD with momentum optimizer is accepted\n",
    "* training set is reduced to the fist 20,000 samples\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* build from the code below that already implements the restrictions to the competition\n",
    "* adapt as seen fit\n",
    "\n",
    "```\n",
    "sgd = optimizers.SGD(lr=eta, decay=1e-6, momentum=0.9)\n",
    "\n",
    "input_train_comptetition = input_train[:20000]\n",
    "labels_train_competition = labels_train[:20000]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "input_train_competition = input_train[:20000]\n",
    "labels_train_competition = labels_train[:20000]\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc_1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "optimizers.SGD(lr=eta, decay=1e-6, momentum=0.9)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train_competition , labels_train_competition,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], evaluation[0])\n",
    "print('test', history.params['metrics'][1], evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating network architecture graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display, SVG\n",
    "# from keras.utils import plot_model\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# # setup network\n",
    "# n_w1 = 64\n",
    "# lr = 3.\n",
    "\n",
    "# input_img = Input(shape=(input_train_shape,), name='input')\n",
    "# x = Dense(n_w1, activation='sigmoid', name='fc_1')(input_img)\n",
    "# x = Dense(n_w1, activation='sigmoid', name='fc_2')(x)\n",
    "# x = Dense(n_w1, activation='sigmoid', name='fc_3')(x)\n",
    "# output = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# sgd = optimizers.SGD(lr=lr)\n",
    "# model = Model(input_img, output)\n",
    "# model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# # save model graph\n",
    "# plot_model(model, to_file='ann_deep.png')\n",
    "\n",
    "# # plot model graph\n",
    "# display(SVG(model_to_dot(model).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "<img src=\"fig/ann_deep.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 1**\n",
    "\n",
    "Examine the effect of scaling the input scamples between 0 and 1, in addition to other typical pre-processing options, such as scaling the samples to $\\mu=0$ and $\\sigma=1$ both pixel-wise and image-wise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
