{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence, Depth and High-dimensional data\n",
    "# Python ANN notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will:\n",
    "- Code in plain NumPy a simple deep feedforward artificial neural network to classify the MNIST dataset.\n",
    "- Implement the stochastic gradient descent learning algorithm and train our network using backpropagation.\n",
    "- Play with the network to better understand the role of each element and acheive better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Preprocessing\n",
    "\n",
    "1. Load the MNIST dataset and reduce the size of the images by two.\n",
    "\n",
    "2. Reshape the training and test images to arrays of size ($N_x,N_{batch}$).  Reshape the training and test labels to one-hot vectors.\n",
    "\n",
    "3. Vizualize an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider a 3-layer network: one input layer of size $N_x$, one hidden layer of size $N_h$, and one output layer of size $N_y$.\n",
    "\n",
    "In vector notation, this gives:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{h} &= \\sigma(\\mathbf{z}^1), \\quad \\mathbf{z}^1 &= \\mathbf{W}^{1}\\mathbf{x}+\\mathbf{b}^{1},\\\\\n",
    "\\hat{\\mathbf{y}} &= \\sigma(\\mathbf{z}^2), \\quad \\mathbf{z}^2 &= \\mathbf{W}^{2}\\mathbf{h}+\\mathbf{b}^{2},\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\sigma$ denotes a sigmoidal point-wise non-linearity and $\\mathbf{x}$ an input vector.\n",
    "\n",
    "We will use a quadratic cost to evaluate the performance of the network:\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{C} = \\frac{1}{2}\\| \\mathbf{y} - \\hat{\\mathbf{y}} \\|^2 = \\frac{1}{2}(\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} - \\hat{\\mathbf{y}}) \n",
    "\\end{eqnarray}\n",
    "\n",
    "where $(\\mathbf{x},\\mathbf{y})$ is a pair of training data.\n",
    "\n",
    "In component form, this is:\n",
    "\\begin{eqnarray}\n",
    "h_i &= \\sigma(\\sum_j W^{1}_{ij}x_j+b^{1}_i), \\\\\n",
    "\\hat{y}_i &= \\sigma(\\sum_j W^{2}_{ij}h_j+b^{2}_i), \\\\\n",
    "\\mathcal{C} &= \\frac{1}{2 n}\\sum_k(\\mathbf{y}_k - \\hat{\\mathbf{y}}_k)^2.\n",
    "\\end{eqnarray}\n",
    "\n",
    "The component form is for your understanding, but coding it up that way would be inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with a 3-layer network where $N_x = 2, N_h=3,$ and $N_y=1$.\n",
    "\n",
    "Write a function called `initialize` that generates the two connection matrices, $W^{1}$ and $W^{2}$, and two biases, $b^1$ and $b^2$. What size should they be? For now, fill them with random numbers drawn from a Gaussian distribution with mean 0, and variance 1.\n",
    "\n",
    "`Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_simple(n_input, n_hidden, n_output):\n",
    "\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = initialize_simple(2,3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[optional] Generalize this function to deep feedforward network with $D$ layers of weights $\\mathbf{W}^1, ... ,\\mathbf{W}^D$ and $D + 1$ layers of neural activity vectors $\\mathbf{x}^0, ... , \\mathbf{x}^D$, with $N^l$ neurons in each layer $l$, so that $\\mathbf{x}^l ∈ R ^ {N^l}$ and $\\mathbf{\\mathbf{W}}^l$ is an $N^l × N^{l−1}$ weight matrix:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{x}^{l} = \\phi(\\mathbf{z}^{l}), \\qquad \\mathbf{z}^{l} = \\mathbf{W}^{l}\\mathbf{x}^{l-1}+\\mathbf{b}^{l}, \\qquad \\text{ for } l = 1,...,D.\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "``` The input to this function should be a list containing the number of neurons in the respective layers of the network.\n",
    "For example, if the list was [196, 50, 30, 10] then itwould be a four-layer network, with the first layer containing 196 neurons, the second layer 50 neurons, the third layer 30 neurons and the last layer 10 neuron.```\n",
    "\n",
    "Tip: use zip, a python built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(sizes):\n",
    "\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = initialize([196,50,30,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Feedforward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `sigmoid` that applies the sigmoidal non-linearity to its input:\n",
    "\n",
    "$$\\sigma(\\mathbf{z}) = \\frac{1}{1 + e^{-\\mathbf{z}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `feedforward` that takes the inputs, $x$ (an array), and the connection matrices, and returns the output, $\\hat{y}$.\n",
    "\n",
    "Use the NumPy's matrix multiplication function, `dot` and the `sigmoid` function that you just wrote.\n",
    "\n",
    "Make sure that your code can simultaneously (and efficiently) evaluate the output for many $x$. The input matrix now is given in the form of a 2-dimensional array $X$ of size $(N_x,N_{\\mathrm{batch}})$.\n",
    "\n",
    "Create some random input of this form and run it through your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward_simple(params, x):\n",
    "    \n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71983476,  0.71180071,  0.69890168,  0.70539463,  0.71049355,\n",
       "         0.72533837,  0.71119558,  0.71194165,  0.71308866,  0.7009229 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward_simple(p, np.random.randn(2,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[optional] Generalize this function to networks of arbitrary depth. \n",
    "\n",
    "Tip: use zip, a python built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedforward(params, a):\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward(params, x_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 3 artists>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11dc2e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.stem(feedforward(params, x_train)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function called `evaluate` that returns the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.\n",
    "\n",
    "2. Evaluate the naive network, and check that it performs at chance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(params, (x_test, y_test)):\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(params, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to traning our network: given a training set of input/output examples, $(x,y)$, we want to bring the network's output $\\hat{y}$ closer to the desired label $y$.\n",
    "\n",
    "<img src=\"fig/backprop.png\" style=\"width:80%;height:80%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `cost_derivative` that returns the vector of partial derivatives $\\partial C_x / \\partial y $ a for the output activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_derivative(output_activations, y):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `sigmoid_prime` that returns the derivative of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `backprop` that returns a tuple ``(nabla_b1, nabla_b2, nabla_w2, nabla_w2)`` representing the gradient for the cost function C_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients_simple(params, x, y):\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[optional] Generalize: Write a function called `backprop` that returns a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``params['biases']`` and ``params['weights']``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradients(params, x, y):\n",
    "    \n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write a function that implements gradient descent on our simple network using the function we just coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = [x_train, y_train]\n",
    "test_data = [x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[optional] Generalize to networks of arbitrary depth. Explain what computational gain is achieved by gradient descent (compare reverse mode differentiation to forward mode differentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `update_minibatch` that update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mini_batch_simple(params, mini_batch, eta):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[optional] Generalize to networks of arbitrary depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_mini_batch(params, mini_batch, eta):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[optional] Write a generalized function called `SGD` that takes as input: `params, training_data, epochs, mini_batch_size, eta` and performs the optimization.\n",
    "\n",
    "Add an optional `test_data` argument to your function to evaluate the network against the test data after each epoch  and print partial progress printed.  Note that this is useful for tracking progress, but slows things down substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, training_data, epochs, mini_batch_size, eta,\n",
    "        test_data=None):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SGD(params, training_data, epochs=3, mini_batch_size=10, eta=3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(params['weights'][0], aspect = )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Project\n",
    "\n",
    "\n",
    "** Fiddling with our simple network**\n",
    "- write a class called Network that contains and organizes all the above code\n",
    "- vizualize the evolution of the parameters, plot the mean and variance of gradients along learning\n",
    "\n",
    "- optimization: momentum, etc.\n",
    "- activation function: sigmoid (beware of the vanishing gradients on sigmoids), relu (beware of the dying ReLUs)\n",
    "- regularization: penalize large weights, implement dropout (multiply by Bernoulli matrix), etc.\n",
    "- error function: cross-entropy, etc.\n",
    "- learning rate: use recent history, eg. adagrad, adam, etc.\n",
    "- initialization: uniform, gaussian, etc.\n",
    "\n",
    "- what happens if you apply a simple linear transformation to the test data?\n",
    "\n",
    "- explore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
