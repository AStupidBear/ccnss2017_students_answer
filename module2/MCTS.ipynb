{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import definitions of the environments.\n",
    "import RL_worlds as worlds\n",
    "\n",
    "# Import helper functions for plotting.\n",
    "from plot_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, value, params):\n",
    "    if np.random.rand() < 1 - params[\"epsilon\"]: \n",
    "        a = argmax(value[state, :])\n",
    "    else:\n",
    "        a = np.random.randint(params[\"environment\"].n_actions)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_state(params):\n",
    "    \"\"\"\n",
    "    Initialize the state at the beginning of an episode.\n",
    "    Args:\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the initial state.\n",
    "    \"\"\"\n",
    "    if params['environment'].name == 'windy_cliff_grid':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'n_armed_bandit':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cheese_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'cliff_world':\n",
    "        return 0\n",
    "    elif params['environment'].name == 'quentins_world':\n",
    "        return 54\n",
    "\n",
    "def update_state(state, action, params):\n",
    "    \"\"\"\n",
    "    State transition based on world, action and current state.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        action: integer corresponding to the action taken.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding to the next state;\n",
    "        an integer corresponding to the reward received.\n",
    "    \"\"\"\n",
    "    next_state, reward = params['environment'].get_outcome(state, action)\n",
    "    return next_state, reward\n",
    "    \n",
    "def call_policy(state, value, params):\n",
    "    \"\"\"\n",
    "    Call a policy to choose actions, given current state and value function.\n",
    "    Args:\n",
    "        state: integer corresponding to the current state.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters.\n",
    "    Returns:\n",
    "        an integer corresponding action chosen according to the policy.\n",
    "    \"\"\"\n",
    "    # multiple options for policy\n",
    "    if params['policy'] == 'epsilon_greedy':\n",
    "        return epsilon_greedy(state, value, params)\n",
    "    elif params['policy'] == 'softmax':\n",
    "        return softmax(state, value, params)\n",
    "    else: # random policy (if policy not recognized, choose randomly)\n",
    "        return randint(params['environment'].n_actions)\n",
    "\n",
    "def update_value(prev_state, action, reward, state, value, params):\n",
    "    \"\"\"\n",
    "    Update the value function.\n",
    "    Args:\n",
    "        prev_state: an integer corresponding to the previous state.\n",
    "        action: an integer correspoding to action taken.\n",
    "        reward: a float corresponding to the reward received.\n",
    "        state: an integer corresponding to the current state;\n",
    "          should be None if the episode ended.\n",
    "        value: a matrix indexed by state and action.\n",
    "        params: a dictionary containing the default parameters. \n",
    "    Returns:\n",
    "        the updated value function (matrix indexed by state and action).\n",
    "    \"\"\"\n",
    "    if params['learning_rule'] == 'q_learning':\n",
    "        # off policy learning\n",
    "        return q_learning(prev_state, action, reward, state, value, params)\n",
    "    elif params['learning_rule'] == 'sarsa':\n",
    "        # on policy learning\n",
    "        return sarsa(prev_state, action, reward, state, value, params)\n",
    "    else:\n",
    "        print('Learning rule not recognized')\n",
    "\n",
    "def default_params(environment):\n",
    "    \"\"\"\n",
    "    Define the default parameters.\n",
    "    Args:\n",
    "        environment: an object corresponding to the environment.\n",
    "    Returns:\n",
    "        a dictionary containing the default parameters, where the keys\n",
    "            are strings (parameter names).\n",
    "    \"\"\"\n",
    "    params = dict()\n",
    "    params['environment'] = environment\n",
    "    \n",
    "    params['alpha'] = 0.1  # learning rate    \n",
    "    params['beta'] = 10  # inverse temperature    \n",
    "    params['policy'] = 'epsilon_greedy'\n",
    "    params['epsilon'] = 0.05  # epsilon-greedy policy\n",
    "    params['learning_rule'] = 'q_learning'\n",
    "    params['epsilon_decay'] = 0.9\n",
    "    \n",
    "    if environment.name == 'windy_cliff_grid':\n",
    "        params['gamma'] = 0.6  # temporal discount factor\n",
    "    elif environment.name == 'n_armed_bandit':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "    elif environment.name == 'cliff_world':\n",
    "        params['gamma'] = 1.0  # no discounting\n",
    "    elif environment.name == 'cheese_world':\n",
    "        params['gamma'] = 0.5  # temporal discount factor\n",
    "    elif environment.name == 'quentins_world':\n",
    "        params['gamma'] = 0.9  # temporal discount factor\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RL(params, n_episodes = 500, n_steps = 1000):\n",
    "    env = params[\"environment\"]\n",
    "    value = np.zeros((env.n_states, env.n_actions))\n",
    "    rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state = init_state(params)\n",
    "        reward_sum = 0\n",
    "        for step in range(n_steps):\n",
    "            action = call_policy(state, value, params)\n",
    "            next_state, reward = update_state(state, action, params)\n",
    "            reward_sum += reward\n",
    "            value = update_value(state, action, reward, next_state, value, params)  \n",
    "            state = next_state\n",
    "            if next_state == None: break\n",
    "        rewards.append(reward_sum)\n",
    "    return value, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = default_params(worlds.cheese_world())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q = sparse.lil_matrix((params[\"environment\"].n_states, params[\"environment\"].n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "n_steps = 10\n",
    "for episode in range(n_episodes):\n",
    "    state = init_state(params)\n",
    "    reward_sum = 0\n",
    "    for step in range(n_steps):\n",
    "        action = call_policy(state, Q, params)\n",
    "        next_state, reward = update_state(state, action, params)\n",
    "        state = next_state\n",
    "        if next_state == None: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
